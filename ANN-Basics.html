<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>ANN Basics</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">ANN Basics</h1>
</header>
<h1 id="artificial-neural-networks">Artificial neural networks</h1>
<p>Artificial neural networks or short: ANN (german: KÃ¼nstliches Neuronales Netzwerk).</p>
<p>ANNs are no new invention, to the contrary they are comparatively old, but the increasing computation power enables us to calculate bigger and bigger networks, which enables us to attack bigger and bigger problems.</p>
<h2 id="neurons">Neurons</h2>
<p>Since a ANN is a network or collection of neurons, the question occurs &quot;What is a neuron?&quot;.</p>
<p>A neuron is inspired by the structure of a neuron in a human brain. It receives one or more impulses and reacts by outputting a more or less intense signal.</p>
<h3 id="realization">Realization</h3>
<p>A neuron is receiving an input-vector (the impulse):</p>
<p><span class="math display">\[
\overrightarrow{x} = \left(\begin{array}{c}x_{0}\\ x_{1}\\ ...\\ x_{n}\end{array}\right)
\]</span></p>
<p>After the neuron receives the input it starts to evaluate the input, by multiplying each input value with an internally saved <strong>Weight</strong>. When we create a new neural network we normally initialize the weights with random values.</p>
<p><span class="math display">\[
\sum_{i=0}^{n} w_{i} * x_{i}
\]</span></p>
<p>After the input is evaluated the <strong>Offset (Bias)</strong> gets added, the offset is used as additional parameter to increase the efficiency of the network. The resulting value is the internal state z. When we create a new neural network we normally initialize each bias with the value zero (0).</p>
<p><span class="math display">\[
z = (\sum_{i=0}^{n} w_{i} * x_{i})+b
\]</span></p>
<p>This so calculated <strong>internal state</strong> gets then passed on to the <strong>activation function</strong> to calculate the <strong>Activation(y)</strong> of the Neuron.</p>
<p><span class="math display">\[
y =  f(z)
\]</span></p>
<p>In the end the so calculated Activation is passed on to <strong>other neurons</strong>, commonly of the next layer.</p>
<h2 id="activation-function">Activation Function</h2>
<p>After we evaluate the input, we have a value that has no defined range. This value can be any real value (between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(+\infty\)</span>). But we still have to decide if this value is enough to trigger the neuron to fire or not. Therefore we use the activation function to decide for which range of values the connected neurons will consider this neuron to be fired and for which range they will consider the neuron as not fired. There are different kinds of activation functions, which bring different advantages and disadvantages. Sometimes activation functions are even linked to learning algorithms so you have to use the linked activation function to use this learning algorithm.</p>
<p>The most common activity functions are the Sigmoid function, the Hyperbolic Tangent and especially the ReLu (Rectifier Linear Unit) function (max(0,x)).</p>
<figure>
<img src="http://www.cbcity.de/wp-content/uploads/2016/03/ActivationFunctions-770x154.png" title="common activation functions" alt="common activation functions" /><figcaption>common activation functions</figcaption>
</figure>
<h4 id="rectifier-linear-unit">Rectifier linear unit</h4>
<p><span class="math display">\[
f(x)= max(x,0)
\]</span></p>
<p>One of the simplest activation functions, that lets the Neuron only fire if the activation is bigger than zero.</p>
<ul>
<li>value range: <span class="math inline">\([0, +\infty]\)</span></li>
<li>not differentiable everywhere</li>
<li>continuous</li>
</ul>
<h4 id="exponential-linear-unit">Exponential linear unit</h4>
<p><span class="math display">\[
f(x)= \left\{\begin{matrix}
 x&amp; \text{if x}\geq 0 \\
 (e^{x}-1)&amp; \text{otherwise}
\end{matrix}\right.
\]</span></p>
<p>Exponential modification of the Rectifier, so the Neuron also fires when negative values are given (but weaker).</p>
<ul>
<li>value range: <span class="math inline">\([0, +\infty]\)</span></li>
<li>differentiable everywhere</li>
</ul>
<h4 id="softplus">Softplus</h4>
<p><span class="math display">\[
f(x)= log(e^{x}+1)
\]</span></p>
<p>Tries to perform a smooth approximation on the standard relu-functions.</p>
<ul>
<li>returns values in range: <span class="math inline">\([0, +\infty]\)</span></li>
<li>everywhere differentiable</li>
</ul>
<h4 id="softsign">Softsign</h4>
<p><span class="math display">\[
f(x)= \frac{x}{\left | x \right |+1}
\]</span></p>
<ul>
<li>value range: <span class="math inline">\([-1, +1]\)</span></li>
<li>differentiable everywhere</li>
</ul>
<h4 id="sigmoid">Sigmoid</h4>
<p><span class="math display">\[
f(x)= \frac{1}{1+ e^{-x}}
\]</span></p>
<p>One of the most spreaded activation functions and the standard activation function in combination with Backpropagation.</p>
<ul>
<li>value range: <span class="math inline">\([0, 1]\)</span></li>
<li>differentiable everywhere</li>
</ul>
<h4 id="hyperbolic-tangent">hyperbolic tangent</h4>
<p><span class="math display">\[
f(x)= \tanh(x) = \frac{1 + e^{-2x}}{1 - e^{-2x}}
\]</span></p>
<ul>
<li>value range: <span class="math inline">\([-1, 1]\)</span></li>
<li>differentiable everywhere</li>
</ul>
<h2 id="layers">Layers</h2>
<p>A neural network consisting of only one neuron is called a Perceptron, but since such a neural network is limited in its use cases, we tend to use neural networks with several layers.</p>
<p>In general you can differ between three kinds of layers</p>
<ol type="1">
<li><strong>The input layer</strong></li>
</ol>
<p><em>This Layer is passive, doing nothing but passing the input vector on to the hidden layer.</em></p>
<ol start="2" type="1">
<li><strong>The hidden layer</strong></li>
</ol>
<p><em>Every layer that exist between the input layer and output layer is called hidden.</em></p>
<ol start="3" type="1">
<li><strong>The output layer.</strong></li>
</ol>
<p><em>This layer returns the values, that represent the output of the whole neural network, therefore the activation function doesn't get applied to this layer, because the evaluation if a neuron should fire or not is unimportant, since the output of the neuron is used as output for the whole network.</em></p>
<figure>
<img src="http://www.dspguide.com/graphics/F_26_5.gif" alt="Visualization of a simple ANN" /><figcaption>Visualization of a simple ANN</figcaption>
</figure>
<p>A normal neural network has only one input and one output layer, but you can use as many hidden layers as you want. Each additional layer raises the amount of CPU time needed to train and evaluate the network. If your network includes a large number number of layers (i.e., more than 10) it is called a Deep Neural Network.</p>
<p>In a simple artificial neural network each neuron of a layer is connected to every neuron of the next layer. These layers are called Fully Connected Layers.</p>
<h3 id="convolutional-neural-networks">Convolutional Neural Networks</h3>
<p>It's also possible to integrate a case separation into a ANN, this option is common for the extraction of features from a data set. This case separation is called convolution and is realized through a simple function, but it turned out to be a really strong functionality. Such networks are named Convolutional Neural Networks (or short <strong>ConvNets</strong>)</p>
<h2 id="loss-function">Loss function</h2>
<p>We can pass values to our network and calculate an activation based on the random weights but we could do the same thing with a piece of paper and a few dies. But before we implement the most well known ability of an AI, namely to learn, we first need to know evaluate the correctness of the AI.</p>
<h3 id="classification">Classification</h3>
<p>ANN can be used to sort inputs into different classes. A simple classification problem might only have two states (true or false). The ANN therefore either gets the class correct or not. To evaluate your AI you can just count how many correct predictions the AI made (the more the better).</p>
<p>For a classifier it's also useful to use a softmax function as the last layer to transform the actual activations of the previous layer into a probability (from 0 to 1).</p>
<h5 id="softmax-function">Softmax Function</h5>
<p>This function is used to reduce a K-dimensional vector z of arbitrary real numbers:</p>
<p><span class="math display">\[
z = \left(\begin{array}{c}z_{0}\\ z_{1}\\ ...\\ z_{K}\end{array}\right) e.g.: \left(\begin{array}{c}80.06\\ -200\\ -50.2\\ 0\end{array}\right)
\]</span></p>
<p>to a K-dimensional vector of real values between [0, 1] that add up to one.</p>
<p><span class="math display">\[
\sigma(z) = \left(\begin{array}{c} \sigma(z)_{0}\\ \sigma(z)_{1} \\ ...\\ \sigma(z)_{K}\end{array}\right) e.g.: \left(\begin{array}{c}0.5\\ 0.05\\0.05\\ 0.4\end{array}\right)
\]</span></p>
<p><span class="math display">\[
1 = \sum_{i = 0}^{K} \sigma(z)_{i}
\]</span></p>
<p><span class="math display">\[
\text{for example:} ~~ 1 = 0.5 + 0.05 + 0.05 + 0.4
\]</span></p>
<h3 id="regression">Regression</h3>
<p>In this case we expect our network to return an estimated or predicted response (mostly a value or a set of values). We need a different loss function for calculate the loss. Commonly we will therefore use L1 were we just use the <strong>difference between the expected and the returned output</strong>. Mostly to create a more neutral loss we take the <strong>square of the difference</strong> this loss function is then called L2. In some cases it can also become handy to use Cross Entropy as a loss function.</p>
<h2 id="learning">Learning</h2>
<p>After we defined which cases our network got right or wrong, we can start to teach it. Note that you naturally have to first collect fitting training and test data for your network. After that you train your network by giving the network one set of your training data to process. Since we initialized the weights of the neurons with random values it is unlikely that the output is similar to the expected output. To change that we start to tune our network using our loss function. There are several different technics how to do that, I will introduce some of them in a different file.</p>
<h2 id="representation-as-matrix">Representation as Matrix</h2>
<p>It's possible to represent a neural network as sequence of matrices. We extract the weight-vectors of the neurons and assemble them in a matrix. Each column of these matrices represents the weights of one neuron. Each Bias gets extracted into a separate vector that gets added during the evaluation. This principle can be applied to every hidden layer as well as the output layer. <span class="math display">\[n : \text{length input vector} \]</span></p>
<p><span class="math display">\[nH : \text{number hidden neurons} \]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
 w_{1|1}&amp; w_{1|2} &amp; ... &amp; w_{1|n} \\
 w_{2|1}&amp; w_{2|2} &amp; ... &amp; w_{2|n} \\
 ... &amp; ... &amp;  ... &amp; ... \\
 w_{nH|1} &amp; w_{nH|2} &amp; ... &amp; w_{nH|n}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
nO : \text{number of output neurons}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
 w_{1|1}&amp; w_{1|2} &amp; ... &amp; w_{1|nH} \\
 w_{2|1}&amp; w_{2|2} &amp; ... &amp; w_{2|nH} \\
 ... &amp; ... &amp;  ... &amp; ... \\
 w_{nO|1} &amp; w_{nO|2} &amp; ... &amp; w_{nO|nH}
\end{bmatrix}
\]</span></p>
<h2 id="book-recommendations">Book recommendations</h2>
<p><em>The Elements of Statistical Learning</em> by Trevor Hastie, Robert Tibshirani, Jerome Friedman</p>
<p><em>Neural Networks</em> by Raul Rojas (<a href="https://page.mi.fu-berlin.de/rojas/neural/neuron.pdf">available online</a>)</p>
</body>
</html>
